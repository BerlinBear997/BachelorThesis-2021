\chapter{Introduction}

For decades,  high-performance-computing systems have become faster than ever thanks to the powerful multi-core microprocessors. While processors deliver a significant amount of floating-point operations per second, power consumption becomes a major challenge. Fugaku, the world's top supercomputer, consumes 29.89MW to deliver 442 PFlop/s \cite{1}. Consequentially, the energy cost under such power consumption is significant, e.g. 29MW at \$0.12/kWh \cite{26} is \$3480 an hour about over \$30 million per year. Moreover, massive carbon dioxide emissions are noticeable, more manpower is needed to maintain them. As a result, people decided to set power budgets on high-performance computers and scientists tried numerous methods to find the balance between high performance and energy efficiency. For example, the Department of Energy (DOE) of the U.S has set a power budget of 30 MW on one of the exascale computing systems Frontier that yet to come \cite{2}. To achieve such a goal, it is important to find a reliable power management approach.

Supercomputers consist of large amounts of computing nodes. Each node is just like a computer or laptop people daily use except it is more powerful in terms of the computing speed of the processor, memory capacity and GPU. The power consumption mainly comes from those components. The microprocessor is the kernel of a supercomputer. It consists of two components namely core and uncore. In short, the core is responsible for computational works while uncore for non-compute-related works such as data transfer. Each of the components has its own frequency.  Running average power limit (RAPL) is a feature of Intel processors to set power caps to ensure the CPU and memory do not exceed the power budget \cite{3}.  However, RAPL does not aware of whether the application is compute-intensive or memory-intensive. As a result, RAPL may reduce the uncore frequency though the running program is memory bounded, e.g. STREAM benchmark that heavily demands the uncore resources. 

The Roofline model estimates the performance of an application based on its operational intensity. It can distinguish whether an application is memory- or compute-bound. However, the Roofline model assumes perfect overlap between computation and data transfer. In reality, such a case does not always exist because some applications suffer from high memory access latency. It is important to detect the memory latency and analyse how it affects performance.

In this paper, I propose to extend the Roofline model with a method \textit{time interval analysis}, which detects the memory access latency in the runtime and analyses its impact. Then the core and uncore frequency will be modified accordingly to achieve power saving. The method is implemented with the sampling approach, which means it initiates an interrupt after a certain time frame, analyses hardware activities of the time interval and optimizes the performance by modifying the components' frequency. The paper makes the following contributions:
\begin{enumerate}
\item Introduce the hardware architecture of the modern high-performance computers.
\item Analyse the uncore frequency scaling and its impact on performance, power consumption.
\item Extend the Roofline model with time interval analysis. It is able to detect memory latency and reduce the power consumption during runtime. 
\item Evaluate the extended Roofline method with multiple HPC benchmarks on a node to show the power and energy saving.
\end{enumerate}

The remainder of the paper is organized as follows. Section 2 provides scientific backgrounds. Section 3 describes the Roofline model and propose the time interval analysis. Section 4 shows the implementation. Section 5 discusses the experimental results. Section 6 contains related work. Section 7 summarises the work and draws conclusions.
